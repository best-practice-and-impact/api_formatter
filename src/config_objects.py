import datetime
import json
import yaml 
from pathlib import Path
from typing import Union, Optional


##Important Notes:
#We can change the schema (data) "type" into "dataType" because we have Edition.alert.type field in the schema so there might be potential issue in QA. I created a CombinedSchema2 but we can keep using CombinedSchem until we find a relevant bug
# Although Edition.alert.type is enum with defined list of words , the value in some examples is empty ("") which can be interpreted as NO ALERT. We added "" to its schema enum 

#An inistialisation with default values will affect the QA functionality because empty fields filled with default values with the correct format
#default values were change into None to mark them as incomplete for QA
COMBINED_DEFAULT = {
    "Dataset": {
        "id": None,
        "title": None,
        "description": None,
        "topics": [],
        "qmi": {"href": None},
        "contacts": {"name": None, "email": None, "telephone": None},
        "publisher": {"name": None, "href": None}
    },
    "Edition": {
        "edition": None,
        "edition_title": None,
        "quality_designation": None,
        "usage_notes": {"title": [], "note": []},
        "alerts": {"type": None, "description": None},
        "distributions": {"title": None, "format": None}
    }
}




class MetadataConfig:
    """
    Stores, manages, and validates metadata for a dataset, with built-in quality assurance (QA) functionality.
    This class enables storage, retrieval, and modification of dataset metadata, and supports validation against a JSON schema.
    It is designed to facilitate both programmatic and human-readable QA workflows.

    Attributes
    ----------
    _metadata : dict
        Dictionary containing all defined fields for the dataset metadata, initialized with mostly None or empty values.
    _schema : dict
        The JSON schema (as a dictionary) used for validating the metadata.
    errors : list of str
        List of validation errors generated by the last call to `validate()`. Only present after validation.

    Methods
    -------
    import_from_dict(new_metadata)
        Imports metadata from an external dictionary, updating only recognized fields.
    get(key)
        Retrieves the value associated with the specified metadata key.
    set(nested_path, value)
        Sets the value for the specified metadata key, supporting nested paths.
    load_metadata_from_file(config_path)
        Loads and imports metadata from a JSON or YAML file.
    export_to_json(title, file_path)
        Exports the current metadata to a JSON file.
    load_json(file_path)
        Loads and parses a JSON file from disk, returning a dictionary.
    check_type(value, schema)
        Validates a value against a type and (optionally) enum specified in the schema.
    validate(metadata=None, schema=None, path="")
        Recursively validates metadata against the schema, collecting errors.
    print_QA_errors()
        Prints validation errors in a human-readable format.
    get_errors()
        Returns the list of errors from the last validation, or an empty list if none exist.

    Examples
    --------
    >>> cfg = MetadataConfig("schema.json", COMBINED_DEFAULT)
    >>> cfg.import_from_dict({"title": "Sample", "type": "Research"})
    >>> cfg.validate()
    >>> cfg.print_QA_errors()
    """
    def __init__(self,schema:Union[str, dict], default_metadata):
        """
        Initializes a MetadataConfig instance with default metadata fields and loads the schema for validation.

        Parameters
        ----------
        schema : Union[str, dict]
            File path to a JSON schema, or a dictionary representing the schema.
            If the schema was previously defined, the file can be retrieved from the data folder.

        Raises
        ------
        TypeError
            If the loaded schema object is not a dictionary.
        """
        
        self._metadata = default_metadata.copy()
        #If it's a file path, load it. Otherwise, assume it's already a dict.
        self._schema = self.load_json(schema) if isinstance(schema, str) else schema
        #Defensive: check types after assignment
        if not isinstance(self._schema , dict):
            raise TypeError("Schema must be a dict or a JSON file that parses to a dict.")
            
    def import_from_dict(self, new_metadata: dict):
        """
        Import metadata from a pre-existing dictionary, updating only recognized fields.

        Parameters
        ----------
        new_metadata : dict
            External dictionary to be imported.

        Raises
        ------
        KeyError
            If a key in the external dictionary is not part of the metadata dictionary.
        """
        for key, value in new_metadata.items():
            if key in self._metadata.keys():
                self.set(key, value)
            else:
                raise KeyError(f"Invalid config key: '{key}'.\n"
                    f"Allowed keys are: {list(self._metadata.keys())}"
                    ) 

    def get(self, key: str):
        """
        Retrieve the value of the corresponding key within the metadata.
        Supports nested keys using dot notation (e.g., "Edition.edition").
        Parameters
        ----------
        key : str
            Key used to index metadata, possibly using dot notation for nested keys.
        Returns
        -------
        object
            Value of the input key.
        Raises
        ------
        KeyError
            If the input key is not a field in the metadata.
        """
        keys = key.split(".")
        value = self._metadata

        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                raise KeyError(f"'{key}' is not a valid config option")

        return value



    def set(self, nested_path: str, value):
        """
        Set or update the value for a specific field in the metadata, supporting nested paths.

        Parameters
        ----------
        nested_path : str
            Metadata field path to update, using dot notation for nested fields (e.g., "contacts.email").
        value : object
            New value for the field.

        Returns
        -------
        object
            Validated value if successful.

        Raises
        ------
        ValueError
            If validation fails for the provided value.
        KeyError
            If a key in the path is not valid according to the schema.
        """
        #If users accidentally include spaces in the path (e.g., "contacts. email"), it could cause hard-to-debug issues.
        keys=[key.strip() for key in nested_path.split(".")]
        current_metadata=self._metadata
        current_schema=self._schema
        try:
            # Start at the root schema level, which has "properties"
            # current_schema = current_schema.get("properties", {})
            for i, key in enumerate(keys):
                if key not in current_metadata:
                        current_metadata[key]={}
                # Final key: validate and set
                if i==len(keys)-1:
                    validated_value=self.initial_validate_and_build(key, value, current_schema["properties"])
                    current_metadata[key] = validated_value             
                # Intermediate key: go deeper 
                else:
                    # Intermediate key: go deeper
                    if "properties" not in current_schema or  key not in current_schema["properties"]:
                        raise KeyError(f"'{key}' is not a valid field in the schema.")
                    # Ensure the current key has a 'properties' attribute before proceeding
                    if "properties" not in current_schema["properties"][key]:
                        raise KeyError(f"'{key}' does not have nested properties in the schema.")
                    current_metadata=current_metadata[key]
                    #properties for other fields inside the current field
                    current_schema=current_schema["properties"][key]
            return validated_value
        # except ValueError as ve:
        #     raise ValueError(str(ve))

        # except KeyError as ke:
        #     raise KeyError(str(ke))
        except ValueError as ve:
            raise ve

        except KeyError as ke:
            raise KeyError(f"Key error for path '{nested_path}': {str(ke)}")


    def initial_validate_and_build(self,key: str, value, schema,full_path=None):
        """
        Recursively validate a value against the schema (supports enums, dates, and nested objects).

        Parameters
        ----------
        key : str
            Metadata field name.
        value : object
            Value to validate. Values for nested fields should be dictionaries.
        schema : dict
            Schema definition for validation.

        Returns
        -------
        object or None
            Validated value (possibly transformed), or None if validation fails.
        """
        if key not in schema:
            raise KeyError(f"{key} is not a valid key in the schema.")
        
        # Build the full path for error reporting
        if full_path is None:
            current_path=key
        else:
            current_path=f"{full_path}.{key}" if full_path else key
        #look into the values in case if there's nested values (or another schema)
        key_schema = schema[key]

        #ENUM field
        if "enum" in key_schema:
            allowed_values=key_schema["enum"]
            if value not in allowed_values:
                raise  ValueError(f"Validation error for path '{current_path}': Value '{value}' is not valid for '{key}'. Possible choices are: {allowed_values}")
            

        #DATETIME field
        elif key_schema.get("type")=="datetime":
            if isinstance(value, datetime.datetime):
                # Convert datetime to string for the the final json file
                value = value.strftime("%d/%m/%Y")  
            if not isinstance(value, str):
                raise ValueError(f"Validation error for path '{current_path}': {value} for {key} is not a string. Please use string format") 
            try:
                #Only validate, don't convert
                #checks if the string value matches the date format "dd/mm/yyyy"
                datetime.datetime.strptime(value, "%d/%m/%Y")
            except ValueError:
                raise ValueError(f"Validation error for path '{current_path}': Value '{value}' is the wrong datetime format for '{key}'. Try 'dd/mm/yyyy'.")

        #NESTED OBJECT
        elif key_schema["type"]=="object" and "properties" in key_schema:
            if not isinstance(value, dict):
                raise ValueError(f"Validation error for path '{current_path}': Value for '{key}' expects an object/dict.")
            #iterating through all keys and values inside the nested field
            validated_results={}
            errors=[]
            for subkey, subval in value.items():
                if subkey not in key_schema["properties"]:
                    errors.append(KeyError(f"'{subkey}' is not a valid key in the schema for '{key}'."))
                    #If the subkey is invalid, you still proceed to validate it, which may raise a KeyError again. You should continue after appending the error
                    continue
                try:
                    # Recursively set each property in the nested object
                    nested_result=self.initial_validate_and_build(subkey,subval,key_schema["properties"],current_path)
                    validated_results[subkey]=nested_result
                #early exit
                #If a subfield validator returns None, you should abort the entire nested set to avoid partial writes.
                except (ValueError, KeyError) as e:
                    # Don't re-wrap the error, just let it bubble up
                    errors.append(e)
            
            if errors:        
                error_messages = "\n  - ".join(str(e) for e in errors)
                raise ValueError(f"{error_messages}")


                
            #We need to return validated_results so that each step of the recursion can pass the fully validated (and possibly transformed) data up to its parent call
            return validated_results
        
        #Default: assign as is 
        else:
            #BASE CASE: leaf assignment for other types
            return value
        

    def load_metadata_from_file(self, config_path: str):
        """
        Load metadata from a JSON or YAML file and import it into the instance.

        Parameters
        ----------
        config_path : str
            Path to the configuration file (should be .json, .yaml, or .yml).
            This should be raw string not normal string (r'\' as separator preferably)

        Returns
        -------
        dict
            The loaded metadata dictionary.

        Raises
        ------
        FileNotFoundError
            If the specified file does not exist.
        ValueError
            If the file format is unsupported or parsing fails.
        """
        #check file extension
        format = config_path.split(".")[-1].lower()
        verified_config_path = Path(config_path)
        
        if not verified_config_path.exists():
            raise FileNotFoundError(f"Configuration file not found: {verified_config_path}")
        
        #load the file content based on format
        try:
            with open(verified_config_path, 'r') as file:
                if format == 'json':
                    loaded_raw_metadata = json.load(file)
                elif format in ['yaml', 'yml']:
                    loaded_raw_metadata = yaml.safe_load(file)
                else:
                    raise ValueError(f'Unsupported file format: {format}. Only "json" and "yaml" are supported.')
        except json.JSONDecodeError as e:
            raise ValueError(f'Error parsing JSON file: {e}')
        except yaml.YAMLError as e:
            raise ValueError(f'Error parsing YAML file: {e}')
    
        # it only validates input dictionary and updates the metadata attribute of the instance
        self.import_from_dict(loaded_raw_metadata)

        return loaded_raw_metadata
    
    def export_to_json(self,title, file_path: str = '/api_formatter/results'):
        """
        Export the dataset metadata to a JSON file.

        Parameters
        ----------
        file_path : str, optional
            The directory path for where the JSON file will be stored (default: '/api_formatter/results').
        """
        #changed title instance with the title method instance avoding any conflict with the new metadata fields
        with open(f"{file_path}/{title}_metadata.json", 'w') as fp:
            json.dump(self._metadata, fp)
    #removing this because we might not have title or id key in new metadata fields
    # def __str__(self):
    #     """
    #     Return a string representation of the dataset, including title and ID.

    #     Returns
    #     -------
    #     str
    #         String representation of the dataset.
    #     """
    #     return f'Dataset: {self._dataset_metadata["title"]}, ID: {self._dataset_metadata["id"]}'


    def load_json(self,file_path:str):
        """
        Load and parse a JSON schema file.

        Parameters
        ----------
        file_path : str
            Path to the JSON file to load.

        Returns
        -------
        dict
            Parsed JSON schema.

        Raises
        ------
        FileNotFoundError
            If the specified file does not exist.
        json.JSONDecodeError
            If the file is not valid JSON.
        """
        # Load your schema (as shown in your message)
        with open(file_path) as f:
            data = json.load(f)
        return data
    

    def check_type(self,value, schema):
        """
        Validates that a value matches the expected type and, if specified, allowed values (enums) from a schema definition.

        Supported types (from schema 'type' field):
            - 'string'         : Must be a Python str.
            - 'integer'        : Must be a Python int.
            - 'array'          : Must be a Python list, with recursive validation for 'items'.
            - 'object'         : Must be a Python dict, with recursive validation for 'properties'.
            - 'pathlib.Path'   : Must be a Python str or pathlib.Path.
            - 'datetime'       : Must be a string in "%d/%m/%Y" format.
            - 'enum'           : If present, value must be in schema["enum"].

        If the schema includes an 'enum', the value must also be present in the allowed list.

        Parameters
        ----------
            value: The value to validate.
            schema (dict): The schema definition for this property (should include at least 'type', optionally 'enum', and for arrays, 'items').

        Returns
        -------
            bool: True if the value matches the expected type and enum (if defined), False otherwise.

        Note:
            - For arrays, only supports homogeneous lists and validates each item recursively using the 'items' schema.
            - For objects, recursively validates dictionary keys/values using 'properties'.
            - To support new types, extend this method accordingly.
        """

        #Passing the schema (not just a type) gives your function all the 
        #information it needs to properly and recursively validate any structure defined in JSON Schema.
        data_type=schema.get('type')
        if data_type == "string":
            if not isinstance(value, str):
                #return isinstance(value, str)) exit the function immediately.
                return False
        elif data_type == "integer":
            if not isinstance(value, int):
                return False
        elif data_type == "array":
            if not isinstance(value, list):
                return False
            # Check each item type in the list using the items schema
            item_schema=schema.get("items",{})
            #Recursively check each item against its schema
            if not all(self.check_type(item, item_schema) for item in value):
                return False
        elif data_type == "pathlib.Path":
            if not (isinstance(value, str) or isinstance(value, Path)):
                return False
        #not sure if we get datetime python format or string in the drafted metdata so we check both scenarios
        elif data_type=="datetime":
            if not isinstance(value, str):
                    #only accept strings that match the format.
                    #convert datetime.datetime to a string before calling this function, which we did in initial_validate_and_build
                    return False
            elif isinstance(value,str):
                try: 
                    datetime.datetime.strptime(value,"%d/%m/%Y")
                except ValueError:
                    print(f'{value} is the wrong datetime format. Try "dd/mm/yyyy".')
                    return False
            else:
                return False
        #UNKNOWN TYPE: safer to return False than True
        #If the "type" is not one of those you explicitly handle (like "string", "integer", "array", etc.), the code reaches the final fallback line
        else:
            return False
        #we only use the schema for enums although we have custom variable enum functions
        #because the schema is more flexible when we have to change the enum variables
        if "enum" in schema:
            return value in schema['enum']
        
        return True
    
    #we will have recursive calls in this method so should define instance in case of recurisve calls otherwise the class instance will be used
    def validate(self,metadata:Optional[dict] = None, schema:Optional[dict] = None, path=""):
        """
        Recursively validate the metadata dictionary against the schema, collecting all errors.

        Parameters
        ----------
        metadata : dict, optional
            Metadata to validate (default: instance metadata).
        schema : dict, optional
            Schema to validate against (default: instance schema).
        path : str, optional
            Nested property path for error reporting (internal use).

        Returns
        -------
        list of str
            List of validation error messages.
        """
        if metadata is None:
            metadata=self._metadata
        if schema is None:
            schema=self._schema 
        #If you use a local variable like errors = [] inside validate and pass it along or return it, 
        # each call (including recursive calls) works on its own error list and aviod being overwritten unlike when it's an instance vraiable
        errors=[]
        #extact properties and required values
        props = schema.get("properties", {})
        required = schema.get("required", [])
        # Now check nested required fields for objects (fields with "properties").
        for req_key in required:
            if req_key not in metadata:
                errors.append(f"Missing required field: {path}{req_key}")

        #check all the schema's keys and values in the properties field recursively
        for key, val_schema in props.items():
            if key not in metadata:
                continue  # If a property key (key) is not present in the metadata then next iteration              
            val = metadata[key]
            # iterating through nested objects if there's nested properties object in the current propreties
            if "properties" in val_schema:
                errors += self.validate(val, val_schema, path + key + ".")
            #If it's a leaf (last layer), it type-checks the value.
            elif "type" in val_schema:
                # Type check
                #we shouldn't pass schema (the whole schema for the object) instead of val_schema (the schema for this property) to check_type
                if not self.check_type(val, val_schema):
                    print(val_schema)
                    #check the datasettype errors
                    if "enum" in val_schema:
                        errors.append(
                            f"Incorrect dataset type for {path}{key}: allowed types are {val_schema['enum']}, but got {repr(val)}"
                        )
                    elif "items" in val_schema:
                        errors.append(
                            f"Incorrect item type for {path}{key}: allowed types are {val_schema['items']["type"]}, but got {repr(val)}"
                        )
                    
                    #other errors
                    else:
                        errors.append(
                            f"Incorrect type for {path}{key}: expected {val_schema['type']}, but got {type(val).__name__}"
                        )  
        # Optionally keep for later
        self.errors=errors
        return errors      
    

    def print_QA_errors(self):
        """
        Print the results of the most recent quality assurance (QA) validation in a human-readable format.
        If no validation has been run, indicates that validation has passed by default.
        Note: 
           - Always run validation to refresh self.errors
        """
        #First, makes sure the attribute exists, so the next check is safe.
        #Then, checks if that attribute is non-empty/truthy.
        #Prevents an AttributeError if self.errors hasn’t been set yet.
        #this condition prevent any usage of the method before validation
        if hasattr(self,'errors') and self.errors:
            print("Validation failed with errors:")
            for e in self.errors:
                print("-", e)
        else:
            print("Validation passed!")


    def get_errors(self):
        """
        Retrieve the list of validation errors from the most recent validation run.

        Returns
        -------
        list of str
            List of validation error messages, or an empty list if none exist.
        """
        return getattr(self,'errors',[])

